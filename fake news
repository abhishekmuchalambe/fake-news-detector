# Fake News Detector in Python
# Beginner-Friendly Code (~300 lines)
# -----------------------------------
# This program will:
# 1. Load a dataset of news articles
# 2. Preprocess text (cleaning, stopwords, stemming)
# 3. Convert text to numbers (TF-IDF)
# 4. Train a classifier to detect fake news
# 5. Test the classifier and show accuracy
# 6. Save and load the model for reuse

# Import libraries
import pandas as pd
import numpy as np
import string
import joblib

# NLP libraries
import nltk
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer

# Machine learning tools
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.metrics import accuracy_score, confusion_matrix

# Download stopwords (only first time)
nltk.download('stopwords')

# ----------------------------------------------------
# STEP 1: Load Dataset
# ----------------------------------------------------
# For demo purposes, we assume you have a CSV file: "news.csv"
# The CSV should have at least two columns: ["text", "label"]
# label = "FAKE" or "REAL"

print("\nLoading dataset...")

try:
    df = pd.read_csv("news.csv")
    print("Dataset loaded successfully!")
except FileNotFoundError:
    print("Error: news.csv not found! Please place your dataset in the same folder.")
    # To make this code runnable, let's create a small sample dataset
    sample_data = {
        "text": [
            "Donald Trump says the economy is booming.",
            "Scientists confirm COVID-19 is a hoax.",
            "NASA discovers water on Mars.",
            "Bill Gates admits creating coronavirus in lab."
        ],
        "label": ["REAL", "FAKE", "REAL", "FAKE"]
    }
    df = pd.DataFrame(sample_data)

# Show first 5 rows
print("\nSample of dataset:")
print(df.head())

# ----------------------------------------------------
# STEP 2: Text Preprocessing
# ----------------------------------------------------

# Create a stemmer object
stemmer = PorterStemmer()

# Get English stopwords
stop_words = set(stopwords.words('english'))

# Function to clean text
def clean_text(text):
    # Convert to lowercase
    text = text.lower()
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Tokenize (split into words)
    words = text.split()
    # Remove stopwords and apply stemming
    words = [stemmer.stem(word) for word in words if word not in stop_words]
    # Join back into sentence
    return " ".join(words)

print("\nCleaning text data...")
df['clean_text'] = df['text'].apply(clean_text)

print("Cleaned sample:")
print(df[['text', 'clean_text']].head())

# ----------------------------------------------------
# STEP 3: Convert text to numbers (TF-IDF)
# ----------------------------------------------------

print("\nConverting text to numeric features...")

vectorizer = TfidfVectorizer(max_features=5000)
X = vectorizer.fit_transform(df['clean_text']).toarray()
y = df['label']

print("Feature matrix shape:", X.shape)

# ----------------------------------------------------
# STEP 4: Train-Test Split
# ----------------------------------------------------

print("\nSplitting dataset into train and test sets...")

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training size:", X_train.shape)
print("Testing size:", X_test.shape)

# ----------------------------------------------------
# STEP 5: Train Classifier
# ----------------------------------------------------

print("\nTraining the PassiveAggressiveClassifier...")

model = PassiveAggressiveClassifier(max_iter=1000)
model.fit(X_train, y_train)

print("Model training complete!")

# ----------------------------------------------------
# STEP 6: Evaluate Model
# ----------------------------------------------------

print("\nEvaluating model...")

# Predictions
y_pred = model.predict(X_test)

# Accuracy
acc = accuracy_score(y_test, y_pred)
print("Accuracy:", acc)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred, labels=["FAKE", "REAL"])
print("\nConfusion Matrix:")
print(cm)

# ----------------------------------------------------
# STEP 7: Save and Load Model
# ----------------------------------------------------

print("\nSaving model and vectorizer...")

joblib.dump(model, "fake_news_model.pkl")
joblib.dump(vectorizer, "vectorizer.pkl")

print("Model saved as fake_news_model.pkl")
print("Vectorizer saved as vectorizer.pkl")

# Load back to test
print("\nLoading saved model...")

loaded_model = joblib.load("fake_news_model.pkl")
loaded_vectorizer = joblib.load("vectorizer.pkl")

# ----------------------------------------------------
# STEP 8: Test with Custom Input
# ----------------------------------------------------

def predict_news(news):
    # Clean the news
    cleaned = clean_text(news)
    # Transform with vectorizer
    features = loaded_vectorizer.transform([cleaned]).toarray()
    # Predict
    prediction = loaded_model.predict(features)[0]
    return prediction

print("\nTesting custom news samples...")

news_samples = [
    "The government has announced new education reforms.",
    "Aliens built the pyramids in Egypt last year."
]

for sample in news_samples:
    result = predict_news(sample)
    print(f"News: {sample}\nPrediction: {result}\n")

print("\nFake News Detector is ready!")
